{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import dependencies\n",
    "\n",
    "`requests`  \n",
    "Because `arxiv` is not a dynamic web page (maybe server-side rendering is used), I could scrap all the contents simply by using `requests` module.\n",
    "\n",
    "`BeautifulSoup`  \n",
    "is used to parse the html element easily.\n",
    "\n",
    "`typing`  \n",
    "Python is dynamically typed language.  \n",
    "To avoid error-prone non-typed code, I used `typing` module.\n",
    "\n",
    "`pandas`  \n",
    "provides useful data structure `DataFrame`.  \n",
    "is used to neatly express the parsed data.\n",
    "\n",
    "`threading`  \n",
    "Scrapping all the content (almost 120,000) in one thread is too slow.  \n",
    "I created 20 threads to scrap the content rapidly.\n",
    "\n",
    "`queue`  \n",
    "is used to exchange data safely in multi-thread environment.  \n",
    "> [from python document - queue](https://docs.python.org/3/library/queue.html#module-queue)  \n",
    "> The queue module implements multi-producer, multi-consumer queues. It is especially useful in threaded programming when information must be exchanged safely between multiple threads. The Queue class in this module implements all the required locking semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import typing as T\n",
    "import pandas as pd\n",
    "import threading as th\n",
    "import queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERMS = [\"diffusion\"] # the terms to search for\n",
    "\n",
    "YEAR = 1980 # the year to start searching from\n",
    "\n",
    "SEARCH_SIZE = 200 # the maximum number of results to return (arxiv limit)\n",
    "\n",
    "URL_FORMAT_HEAD = \"https://arxiv.org/search/advanced?advanced=\"\n",
    "\n",
    "URL_FORMAT_TAIL = (\n",
    "    \"&classification-physics_archives=all&classification-include_cross_list=include&date-filter_by=specific_year&date-year={year}\"\n",
    "    + f\"&date-from_date=&date-to_date=&date-date_type=submitted_date&abstracts=show&size={SEARCH_SIZE}\"\n",
    "    + \"&order=announced_date_first&start={index}\"\n",
    ")\n",
    "\n",
    "N_THREADS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define parsed data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivResult:\n",
    "    id: int\n",
    "    title: str\n",
    "    authors: T.List[str]\n",
    "    link: str\n",
    "    tags: T.List[str]\n",
    "    originally_announced_at: str\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        id: int,\n",
    "        title: str,\n",
    "        authors: T.List[str],\n",
    "        link: str,\n",
    "        tags: T.List[str],\n",
    "        originally_announced_at: str,\n",
    "    ):\n",
    "        self.id = id\n",
    "        self.title = title\n",
    "        self.authors = authors\n",
    "        self.link = link\n",
    "        self.tags = tags\n",
    "        self.originally_announced_at = originally_announced_at\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"ArxivResult(id={self.id}, title={self.title}, authors={self.authors}, link={self.link}, tags={self.tags}, originally_announced_at={self.originally_announced_at})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define used functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get\n",
    "\n",
    "This is just wrapper function of the requests.get func.  \n",
    "But, if I want to use another module (like selenium) to scrap, I only need to change this function.  \n",
    "**(Change of the dependency will not affect whole code!!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(url: str):\n",
    "    response = requests.get(url)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_url, get_arxiv_results\n",
    "\n",
    "`get_url`  \n",
    "combines `year`, `start_index` (start index to fetch), `terms`, and return proper url to search.\n",
    "\n",
    "`get_arxiv_results`  \n",
    "searches and retrieves arxiv results via url returned by get_url function.  \n",
    "if the param `result_queue` is not None, puts the result into the queue (for multi-threading usecase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(year: int, start_index: int, terms: T.List[str]):\n",
    "    assert len(terms) >= 1\n",
    "    terms = terms.copy()\n",
    "    url = URL_FORMAT_HEAD\n",
    "    term = terms.pop(0)\n",
    "    url += f\"&terms-0-operator=AND&terms-0-term={term}&terms-0-field=all\"\n",
    "    for i, term in enumerate(terms):\n",
    "        url += f\"&terms-{i+1}-term={term}&terms-{i+1}-operator=OR&terms-{i+1}-field=all\"\n",
    "    url += URL_FORMAT_TAIL.format(year=year, index=start_index)\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_arxiv_results(\n",
    "    year: int,\n",
    "    start_index: int,\n",
    "    item_index: int,\n",
    "    terms: T.List[str],\n",
    "    result_queue: T.Optional[queue.Queue] = None,\n",
    ") -> T.List[ArxivResult]:\n",
    "    results = []\n",
    "\n",
    "    url = get_url(year, start_index, terms)\n",
    "    html = get(url)\n",
    "    bs = BeautifulSoup(html, \"html.parser\")\n",
    "    li_results = bs.find_all(\"li\", {\"class\": \"arxiv-result\"})\n",
    "    for i, row in enumerate(li_results):\n",
    "        id = item_index + i + 1\n",
    "        title = row.find(\"p\", {\"class\": \"title\"}).text.strip()\n",
    "        authors = list(\n",
    "            map(lambda author_tag: author_tag.text.strip(), row.find(\"p\", {\"class\": \"authors\"}).find_all(\"a\"))\n",
    "        )\n",
    "        link = row.find(\"p\", {\"class\": \"list-title\"}).find(\"a\").attrs[\"href\"].strip()\n",
    "        tags = list(\n",
    "            map(lambda tag: tag.text.strip(), row.find(\"div\", {\"class\": \"tags\"}).find_all(attrs={\"class\": \"tag\"}))\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            originally_announced_at = row.find(\"p\", {\"class\": \"is-size-7\"}).text.strip()\n",
    "        except:\n",
    "            originally_announced_at = None\n",
    "\n",
    "        results.append(ArxivResult(id, title, authors, link, tags, originally_announced_at))  # type: ignore\n",
    "\n",
    "    if result_queue is not None:\n",
    "        result_queue.put(results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main function\n",
    "\n",
    "1. repeat 2-5 until all the contents parsed.\n",
    "2. parse `N_THREADS` pages. One thread will be in charge of one page at a time.\n",
    "3. wait all threads to be finished.\n",
    "4. combine the results. if no results are found, stop the loop.\n",
    "5. extend the result DataFrame and export as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    search_idx = 0\n",
    "    item_idx = 0\n",
    "    arxiv_results = []\n",
    "\n",
    "    while True:\n",
    "        print(f\"year: {YEAR}\")\n",
    "        queues = [queue.Queue() for _ in range(N_THREADS)]\n",
    "        threads = []\n",
    "        for i in range(N_THREADS):\n",
    "            thread = th.Thread(target=get_arxiv_results, args=(YEAR, search_idx, item_idx, TERMS, queues[i]))\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "            search_idx += SEARCH_SIZE\n",
    "            item_idx += SEARCH_SIZE\n",
    "\n",
    "        prev_size = len(arxiv_results)\n",
    "\n",
    "        for i in range(N_THREADS):\n",
    "            threads[i].join()\n",
    "            arxiv_results.extend(queues[i].get())\n",
    "\n",
    "        try:\n",
    "            item_idx = arxiv_results[-1].id\n",
    "        except:\n",
    "            item_idx = 0\n",
    "\n",
    "        if len(arxiv_results) == prev_size:\n",
    "            YEAR += 1\n",
    "            search_idx = 0\n",
    "            continue\n",
    "\n",
    "        pd.DataFrame(\n",
    "            [\n",
    "                [\n",
    "                    arxiv_result.id,\n",
    "                    arxiv_result.title,\n",
    "                    arxiv_result.authors,\n",
    "                    arxiv_result.link,\n",
    "                    arxiv_result.tags,\n",
    "                    arxiv_result.originally_announced_at,\n",
    "                ]\n",
    "                for arxiv_result in arxiv_results\n",
    "            ],\n",
    "            columns=[\"id\", \"title\", \"authors\", \"link\", \"tags\", \"originally_announced_at\"],\n",
    "        ).to_csv(f\"arxiv_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Homework-yUppmDQM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
